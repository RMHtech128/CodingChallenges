import torch
import torchvision
from read_dataset import read_dataset_det

#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")


# RuntimeError: Attempting to deserialize object on a CUDA device 
# but torch.cuda.is_available() is False. 
# If you are running on a CPU-only machine, 
# please use torch.load with map_location=torch.device('cpu') 
# to map your storages to the CPU.


# print(device)

BATCH_SIZE = 4
#trainloader, testloader = read_dataset_det(['C:/Users/*******************/Defect_Detection/Train',
#                                          'C:/Users/*******************/Defect_Detection/Test'], (300,400))

trainloader, testloader = read_dataset_det(['C:/Users/*******************/Defect_Detection/Train',
                                          'C:/Users/*******************/Defect_Detection/Test'], (300,400))
									  
										  
classes = ('Defective', 'Good')

import matplotlib.pyplot as plt
import numpy as np

# show image

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

print("///////////// importing torch.nn /////////////")

import torch.nn as nn
import torch.nn.functional as F


print(" ///////////// define class Net /////////////")

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.avgpool = nn.AvgPool2d((144,194))
        self.fc1 = nn.Linear(16, 2)

        self.last_conv = lambda x: F.relu(self.conv2(self.pool(F.relu(self.conv1(x)))))

    def forward(self, x):
        x = self.last_conv(x)
        x = self.avgpool(x)
        x = x.view(-1, 16)
        x = self.fc1(x)
        return x
	     
print("///////////// Instantiate Net and run //////////////// ")

net = Net()
print(net)
print(len(list(net.parameters())))

print("///////////// Load the model path //////////////// ")
LOAD_PATH = './models/defect_net4_25epochs.pth'
print("///////////// Set the checkpoint path //////////////// ")
# the following line has an error on non-GPU machines.
checkpoint = torch.load(LOAD_PATH)
# to correct for the error... here is a quick and dirty fix:
# https://stackoverflow.com/questions/56369030/runtimeerror-attempting-to-deserialize-object-on-a-cuda-device

print("///////////// Load the Net state checkpoint //////////////// ")
map_location=torch.device('cpu')

net.load_state_dict(checkpoint['model_state_dict'])
print("///////////// Call net.to(device) //////////////// ")
net.to(device)

print("///////////// import torch.optim  //////////////// ")

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

import time

now = time.time()

epoch = checkpoint['epoch']
for epoch in range(epoch, epoch+3):  
    correct = 0
    total = 0
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the gradients
        optimizer.zero_grad()

        # forward backward optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        with torch.no_grad():
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # print stat analysis
        running_loss += loss.item()
        if i % 20 == 19:    # print every after each 2000 
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            print(outputs, labels)
            running_loss = 0.0
    print('Accuracy on the trained images: %d %%' % (
            100 * correct / total))

print('completed Training')
print(time.time()-now)

dataiter = iter(testloader)
images, labels = dataiter.next()
images = images.to(device)
labels = labels.to(device)

# print images
imshow(torchvision.utils.make_grid(images.cpu()))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(BATCH_SIZE)))

outputs = net(images)
_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
                              for j in range(BATCH_SIZE)))

correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the test images: %d %%' % (
    100 * correct / total))

SAVE_PATH = './models/defect_net4b_3epochs.pth'
torch.save({'epoch' : epoch+1,
            'model_state_dict' : net.state_dict(),
            'optimizer_state_dict' : optimizer.state_dict(),
            'loss' : loss,
            'test' : correct/total*100}, SAVE_PATH)

# use 2-class output
# can't train subsequent epochs on new train/test split - now done randomly split
# apply image augmentation